{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: JPmogran \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://apps.occ.gov/EASearch/Search/Advanced?Search=Charles%20Schwab&StartDateMinimum=02%2F23%2F2016&StartDateMaximum=02%2F23%2F2021&TerminationDateMinimum=&TerminationDateMaximum=&ShowIndividualActionsOnly=false&ShowInstitutionActionsOnly=false&ShowTerminatedActionsOnly=false&ShowActiveOnly=false&Category=&Sort=BankName&AutoCompleteSelection=&CurrentPageIndex=0&ItemsPerPage=10&view=Table\"\n",
    "val1 = input(\"Keyword: \")\n",
    "key = val1.split(\" \")\n",
    "keyword = '+'.join(['%-2s' % (i,) for i in key])\n",
    "url = 'https://google.com/search?q='+keyword\n",
    "\n",
    "f = open(\"{}.txt\".format(val1), \"w\")\n",
    "\n",
    "# Perform the request\n",
    "request = urllib.request.Request(url)\n",
    "\n",
    "# Set a normal User Agent header, otherwise Google will block the request.\n",
    "request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36')\n",
    "raw_response = urllib.request.urlopen(request).read()\n",
    "\n",
    "# Read the repsonse as a utf-8 string\n",
    "html = raw_response.decode(\"utf-8\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The code to get the html contents here.\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all the search result divs\n",
    "divs = soup.select(\"#search div.g\")\n",
    "for div in divs:\n",
    "    # Search for a h3 tag\n",
    "    results = div.select(\"a\")\n",
    "    if results:\n",
    "        links = results[0].get('href')\n",
    "        f.write(links+\"\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: JPMorgan Violation\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "val1 = input(\"Keyword: \")\n",
    "key = val1.split(\" \")\n",
    "keyword = '+'.join(['%-2s' % (i,) for i in key])\n",
    "url = 'https://google.com/search?q='+keyword\n",
    "\n",
    "f = open(\"{}.txt\".format(val1), \"w\")\n",
    "\n",
    "# Perform the request\n",
    "request = urllib.request.Request(url)\n",
    "\n",
    "# Set a normal User Agent header, otherwise Google will block the request.\n",
    "request.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36')\n",
    "raw_response = urllib.request.urlopen(request).read()\n",
    "\n",
    "# Read the repsonse as a utf-8 string\n",
    "html = raw_response.decode(\"utf-8\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The code to get the html contents here.\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all the search result divs\n",
    "divs = soup.select(\"#search div.g\")\n",
    "for div in divs:\n",
    "    # Search for a h3 tag\n",
    "    results = div.select(\"a\")\n",
    "    if results:\n",
    "        links = results[0].get('href')\n",
    "        f.write(links+\"\\n\")\n",
    "\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: U.S. Bancorp\n"
     ]
    }
   ],
   "source": [
    "#Extract links from FTC\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "\n",
    "\n",
    "val1 = input(\"Keyword: \")\n",
    "key = val1.split(\" \")\n",
    "keyword = '%20'.join(['%-2s' % (i,) for i in key])\n",
    "\n",
    "\n",
    "#open files in writing mode\n",
    "f = open(\"{}.txt\".format(val1), \"w\")\n",
    "\n",
    "def request_data(url):\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "# create a list of all pages of search results\n",
    "url_list = []\n",
    "for i in range(10):\n",
    "    text = \"{}\".format(i)\n",
    "    urls = 'https://search.usa.gov/search?affiliate=ftc_prod&page=' + text +\"&query=\" + keyword\n",
    "    url_list.append(urls)\n",
    "list1 = ['2016','2017','2018','2019','2020']\n",
    "title_list = []\n",
    "for urls in url_list:\n",
    "    soup = request_data(urls)\n",
    "\n",
    "    for links in soup.find_all('a'):\n",
    "        data = links.get('href')\n",
    "        if data:\n",
    "            for i in list1:\n",
    "                if i in data:\n",
    "                    f.write(data+\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract links from Federal Reserve\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "\n",
    "\n",
    "val1 = input(\"Keyword: \")\n",
    "key = val1.split(\" \")\n",
    "keyword = '%20'.join(['%-2s' % (i,) for i in key])\n",
    "\n",
    "\n",
    "#open files in writing mode\n",
    "f = open(\"{}.txt\".format(val1), \"w\")\n",
    "\n",
    "def request_data(url):\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "# create a list of all pages of search results\n",
    "url_list = []\n",
    "for i in range(10):\n",
    "    text = \"{}\".format((i)*10)\n",
    "    urls = 'https://www.fedsearch.org/board_public/search?start=' + text +\"&number=10&source=board_pub&text=\" + keyword\n",
    "    url_list.append(urls)\n",
    "list1 = ['2016','2017','2018','2019','2020']\n",
    "title_list = []\n",
    "for urls in url_list:\n",
    "    soup = request_data(urls)\n",
    "\n",
    "    for links in soup.find_all('a'):\n",
    "        data = links.get('href')\n",
    "        if data:\n",
    "            for i in list1:\n",
    "                if i in data:\n",
    "                    f.write(data+\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate links\n",
    "f = open(\"Artichoke Joe's Casino enforcement.txt\",'r')\n",
    "f1 = open(\"Artichoke Joe's Casino.txt\",'w')\n",
    "content = f.read()\n",
    "content_list = content.split(\"\\n\")\n",
    "content_list_new = []\n",
    "\n",
    "for i in content_list:\n",
    "    if i not in content_list_new:\n",
    "        content_list_new.append(i)\n",
    "\n",
    "for i in content_list_new:\n",
    "    f1.write(i+\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo Finance Table scraper\n",
    "page = requests.get(\"https://finance.yahoo.com/quote/TSLA?p=TSLA\")\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "data = soup.find_all(\"tbody\")\n",
    "try:\n",
    "    table1 = data.find_all('tr')\n",
    "except:\n",
    "    table1 = None\n",
    "\n",
    "l = {}\n",
    "u = list()\n",
    "\n",
    "for i in range(0, len(table1)):\n",
    "    try:\n",
    "        table1_td = table1[i].find_all('td')\n",
    "    except:\n",
    "        table1_td = None\n",
    "\n",
    "    l[table1_td[0].text] = table1_td[1].text\n",
    "\n",
    "    u.append(l)\n",
    "    l = {}\n",
    "print(u)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
